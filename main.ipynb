{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, LlavaProcessor, LlavaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id                                          file_path\n",
      "0         1  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "1         2  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "2         3  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "3         4  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "4         5  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "   image_id  class_id\n",
      "0         1         1\n",
      "1         2         1\n",
      "2         3         1\n",
      "3         4         1\n",
      "4         5         1\n",
      "   class_id                  class_name\n",
      "0         1  001.Black_footed_Albatross\n",
      "1         2        002.Laysan_Albatross\n",
      "2         3         003.Sooty_Albatross\n",
      "3         4       004.Groove_billed_Ani\n",
      "4         5          005.Crested_Auklet\n",
      "(11788, 2)\n",
      "(11788, 2)\n",
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the CUB-200-2011 dataset\n",
    "def load_cub_dataset(data_dir):\n",
    "    images = pd.read_csv(os.path.join(data_dir, 'images.txt'), sep=' ', names=['image_id', 'file_path'])\n",
    "    labels = pd.read_csv(os.path.join(data_dir, 'image_class_labels.txt'), sep=' ', names=['image_id', 'class_id'])\n",
    "    classes = pd.read_csv(os.path.join(data_dir, 'classes.txt'), sep=' ', names=['class_id', 'class_name'])\n",
    "    return images, labels, classes\n",
    "data_dir = 'data'\n",
    "\n",
    "images, labels, classes = load_cub_dataset(data_dir)\n",
    "\n",
    "print(images.head())\n",
    "print(labels.head())\n",
    "print(classes.head())\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "def get_clip_features(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img_input = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        img_features = clip_model.encode_image(img_input)\n",
    "    return img_features\n",
    "\n",
    "def get_clip_text_features(text):\n",
    "    text_input = clip.tokenize([text]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    return text_features\n",
    "\n",
    "def get_clip_similarity_score(img_features, text_features):\n",
    "    img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity_score = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
    "    return similarity_score.item()\n",
    "\n",
    "def recognize_bird_species(img_path):\n",
    "    img_features = get_clip_features(img_path)\n",
    "    similarities = []\n",
    "    for class_name in classes['class_name']:\n",
    "        text_features = get_clip_text_features(class_name)\n",
    "        similarity_score = get_clip_similarity_score(img_features, text_features)\n",
    "        similarities.append((class_name, similarity_score))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 001.Black_footed_Albatross\n",
      "GT class: 001.Black_footed_Albatross\n"
     ]
    }
   ],
   "source": [
    "img_id = 1\n",
    "image_name = images.iloc[img_id]['file_path']\n",
    "gt_class = labels.iloc[img_id]['class_id']\n",
    "bird_species = recognize_bird_species(os.path.join(os.path.join(data_dir, 'images'), image_name))\n",
    "print(f'Predicted class: {bird_species}')\n",
    "print(f\"GT class: {classes[classes['class_id'] == gt_class]['class_name'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CLIP on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "with open('clip_predictions.txt', 'w') as f:\n",
    "    for img_id in tqdm(images['image_id']):\n",
    "        image_name = images.iloc[img_id]['file_path']\n",
    "        gt_class = labels.iloc[img_id]['class_id']\n",
    "        gt_class = classes[classes['class_id'] == gt_class]['class_name'][0]\n",
    "        bird_species = recognize_bird_species(os.path.join(os.path.join(data_dir, 'images'), image_name))\n",
    "        f.write(f'{img_id} {int(gt_class == bird_species)}\\n')\n",
    "        accuracy += int(gt_class == bird_species)\n",
    "    accuracy /= len(images)\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'data/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg'\n",
    "img = Image.open(img_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  else:\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\miret\\AppData\\Local\\Temp\\ipykernel_19016\\2979974320.py\", line 3, in <module>\n",
      "    blip_processor = BlipProcessor.from_pretrained(blip_model_path, trust_remote_code=True)\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\transformers\\processing_utils.py\", line 466, in from_pretrained\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\transformers\\processing_utils.py\", line 512, in _get_arguments_from_pretrained\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2029, in from_pretrained\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2261, in _from_pretrained\n",
      "    return tokenizer\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert_fast.py\", line 221, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 111, in __init__\n",
      "    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n",
      "Exception: data did not match any variant of untagged enum ModelWrapper at line 250373 column 3\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\miret\\miniconda3\\envs\\cv\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "blip_model_path = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(blip_model_path, trust_remote_code=True)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(blip_model_path, trust_remote_code=True)\n",
    "\n",
    "def generate_blip_caption(image):\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "    output = blip_model.generate(**inputs)\n",
    "    return blip_processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_blip_caption' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m blip_caption \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_blip_caption\u001b[49m(img)\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLIP-2 Caption:\u001b[39m\u001b[38;5;124m\"\u001b[39m, blip_caption)\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_blip_caption' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "blip_caption = generate_blip_caption(img)\n",
    "\n",
    "print(\"BLIP-2 Caption:\", blip_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_processor = LlavaProcessor.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "\n",
    "def generate_llava_caption(image):\n",
    "    inputs = llava_processor(image, return_tensors=\"pt\")\n",
    "    output = llava_model.generate(**inputs)\n",
    "    return llava_processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_caption = generate_llava_caption(img)\n",
    "print(\"LLaVA Caption:\", llava_caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
