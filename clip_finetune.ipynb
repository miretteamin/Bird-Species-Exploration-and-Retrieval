{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHdbgIf5gsRS",
        "outputId": "72cb36d5-106c-46bf-ff14-aa1ebf70a4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ CUB dataset is available at: /content/drive/My Drive/Uni/Bird Exploration/CUB_200_2011\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/Uni/Bird Exploration/CUB_200_2011\"\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "  print(f\"✅ CUB dataset is available at: {dataset_path}\")\n",
        "else:\n",
        "  print(\"❌ Dataset not found! Run the download script first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_DU95chf0uX",
        "outputId": "13f77a4a-52a9-4b57-de25-a0c23df435a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install ftfy regex tqdm nltk peft wandb weave\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install --upgrade -q accelerate bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXaHFAfxB6oj"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"CLIP_LoRA_Bird_Exploration\",  # Set a project name\n",
        "    name=\"CLIP_LoRA_Training_Run\",  # Run name\n",
        "    id=\"d4sze36t\",\n",
        "    config={\"epochs\": 5, \"learning_rate\": 5e-5, \"batch_size\": 8}  # Track hyperparameters\n",
        ")\n",
        "\n",
        "\n",
        "# ✅ Save Model Function (Fixed)\n",
        "def save_model(epoch):\n",
        "    save_path = f\"/content/drive/MyDrive/Uni/Bird Exploration/clip_lora_trained_epoch_{epoch+1}\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # ✅ Save the entire CLIP + LoRA Model\n",
        "    clip_model.save_pretrained(save_path)\n",
        "\n",
        "    # ✅ Save tokenizer & processor\n",
        "    processor.save_pretrained(save_path)\n",
        "\n",
        "    print(f\"✅ Model saved at {save_path}\")\n",
        "\n",
        "def compute_test_loss(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, encoded_texts in dataloader:\n",
        "            images, encoded_texts = images.to(device), encoded_texts.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs_pos = model.get_text_features(input_ids=encoded_texts)\n",
        "            outputs_img = model.get_image_features(pixel_values=images)\n",
        "\n",
        "            # Ensure batch sizes are equal\n",
        "            min_batch_size = min(outputs_pos.shape[0], outputs_img.shape[0])\n",
        "            outputs_pos, outputs_img = outputs_pos[:min_batch_size], outputs_img[:min_batch_size]\n",
        "\n",
        "            # Contrastive loss\n",
        "            labels = torch.ones(min_batch_size).to(device)\n",
        "            loss = loss_fn(outputs_pos, outputs_img, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)  # Average test loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import wandb\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ✅ Load LoRA Model & Processor\n",
        "save_path = \"/content/drive/MyDrive/Uni/Bird Exploration/clip_lora_trained_final\"  # Adjust if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ Load CLIP Model & Apply LoRA Adapter\n",
        "\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "# clip_model = PeftModel.from_pretrained(clip_model, save_path).to(device)\n",
        "\n",
        "# # ✅ Load Processor\n",
        "# processor = CLIPProcessor.from_pretrained(save_path)\n",
        "\n",
        "# print(\"✅ LoRA Adapter successfully loaded! Resuming training...\")\n",
        "\n",
        "# ✅ Load Dataset\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/Uni/Bird Exploration/CUB_200_2011/llava_captions.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# ✅ Dataset Class for Bird Exploration\n",
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, dataset, root_dir=\"/content/drive/My Drive/Uni/Bird Exploration/CUB_200_2011/images/\"):\n",
        "        self.data = list(dataset.values())\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def chunk_text(self, text, chunk_size=77, overlap=10):\n",
        "        \"\"\"Splits text into overlapping 77-token chunks for CLIP compatibility.\"\"\"\n",
        "        tokens = processor.tokenizer(\n",
        "            text, return_tensors=\"pt\", padding=False, truncation=False\n",
        "        )[\"input_ids\"].squeeze()\n",
        "\n",
        "        # Ensure correct tensor format\n",
        "        if len(tokens.shape) == 1:\n",
        "            tokens = tokens.unsqueeze(0)\n",
        "\n",
        "        chunks = []\n",
        "        for i in range(0, len(tokens[0]), chunk_size - overlap):\n",
        "            chunk = tokens[0, i:i+chunk_size]\n",
        "\n",
        "            # ✅ Ensure every chunk is 77 tokens (Pad if necessary)\n",
        "            if len(chunk) < chunk_size:\n",
        "                padding = torch.zeros(chunk_size - len(chunk), dtype=torch.long)\n",
        "                chunk = torch.cat([chunk, padding])\n",
        "\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks if chunks else [torch.zeros(chunk_size, dtype=torch.long)]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        positive_sample = self.data[idx]\n",
        "\n",
        "        # Load Image\n",
        "        class_label = positive_sample[\"class_label\"]\n",
        "        image_filename = os.path.basename(positive_sample[\"image_path\"])\n",
        "        image_path = os.path.join(self.root_dir, class_label, image_filename)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            image = torch.zeros((3, 224, 224))\n",
        "        else:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Process text (Chunk LLAVA Captions Properly)\n",
        "        text = positive_sample.get(\"llava_text\", \"\").strip()\n",
        "        text_chunks = self.chunk_text(text)\n",
        "\n",
        "        # ✅ Ensure every text chunk has exactly 77 tokens\n",
        "        encoded_texts = torch.stack(text_chunks)\n",
        "\n",
        "        return image, encoded_texts\n",
        "\n",
        "# ✅ Custom Collate Function\n",
        "def collate_fn(batch):\n",
        "    images, text_chunks = zip(*batch)\n",
        "\n",
        "    images_tensor = torch.stack(images).to(device)\n",
        "\n",
        "    # ✅ Ensure batch consistency for text sequences\n",
        "    max_length = 77  # Enforce max token length\n",
        "    texts_tensor = pad_sequence(\n",
        "        [chunk[:max_length] for chunks in text_chunks for chunk in chunks],\n",
        "        batch_first=True, padding_value=0\n",
        "    ).to(device)\n",
        "\n",
        "    return images_tensor, texts_tensor\n",
        "\n",
        "# ✅ Split Dataset into Train (70%), Validation (15%), Test (15%)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "dataset_items = list(dataset.items())\n",
        "train_data, val_data, test_data = random_split(dataset_items, [train_size, val_size, test_size])\n",
        "train_dataset, val_dataset, test_dataset = BirdDataset(dict(train_data)), BirdDataset(dict(val_data)), BirdDataset(dict(test_data))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ✅ Optimizer & Contrastive Loss\n",
        "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=1e-3)\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()\n",
        "\n",
        "# ✅ Training Loop with Validation Loss & Model Saving\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# ✅ Function to Compute Validation Loss\n",
        "def compute_val_loss(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, encoded_texts in dataloader:\n",
        "            images, encoded_texts = images.to(device), encoded_texts.to(device)\n",
        "\n",
        "            outputs_pos = model.get_text_features(input_ids=encoded_texts)\n",
        "            outputs_img = model.get_image_features(pixel_values=images)\n",
        "\n",
        "            min_batch_size = min(outputs_pos.shape[0], outputs_img.shape[0])\n",
        "            outputs_pos, outputs_img = outputs_pos[:min_batch_size], outputs_img[:min_batch_size]\n",
        "\n",
        "            labels = torch.ones(min_batch_size).to(device)\n",
        "            loss = loss_fn(outputs_pos, outputs_img, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)  # Average validation loss\n",
        "\n",
        "# ✅ Resume Training from Epoch 1 to 5\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    clip_model.train()\n",
        "    total_train_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    for step, (images, encoded_texts) in enumerate(progress_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, encoded_texts = images.to(device), encoded_texts.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_pos = clip_model.get_text_features(input_ids=encoded_texts)\n",
        "        outputs_img = clip_model.get_image_features(pixel_values=images)\n",
        "\n",
        "        min_batch_size = min(outputs_pos.shape[0], outputs_img.shape[0])\n",
        "        outputs_pos, outputs_img = outputs_pos[:min_batch_size], outputs_img[:min_batch_size]\n",
        "\n",
        "        labels = torch.ones(min_batch_size).to(device)\n",
        "        loss = loss_fn(outputs_pos, outputs_img, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # ✅ Log validation loss every few steps\n",
        "        if step % 100 == 0:\n",
        "            val_loss = compute_val_loss(clip_model, val_loader, loss_fn)\n",
        "            wandb.log({\"Train Loss\": loss.item(), \"Validation Loss (Step)\": val_loss})\n",
        "            print(f\"Step {step}: Train Loss {loss.item():.4f}, Val Loss {val_loss:.4f}\")\n",
        "\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # ✅ Compute & Log Final Validation Loss\n",
        "    val_loss = compute_val_loss(clip_model, val_loader, loss_fn)\n",
        "    wandb.log({\"Validation Loss (Epoch)\": val_loss})\n",
        "\n",
        "    # ✅ Reduce LR if validation loss stops improving\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # ✅ Save the model at the end of each epoch\n",
        "    save_model(epoch, clip_model, processor)\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "aDphaChfLIKM",
        "outputId": "267d6408-bce4-4ba4-d2c8-2062e11a5af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LoRA Adapter successfully loaded! Resuming training...\n",
            "trainable params: 491,520 || all params: 151,768,833 || trainable%: 0.3239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 0/1179 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (199 > 77). Running this sequence through the model will result in indexing errors\n",
            "Epoch 1/5:  22%|██▏       | 255/1179 [03:45<13:38,  1.13it/s, loss=0.000394]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-92597d1da22d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# Contrastive loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "cbbc3750f26d4943b8b388d678e74f5b",
            "a97b1b792c184ca3af77f4fbb7f52e8a",
            "e2fa15fdf5ad4c9ab71499227b1b407c",
            "9794cbcf53174838bdcadca7abacd91b",
            "29742e3b7b9c49e1a98a6ae329a396ec",
            "85ac3c56be3347a68f545ab3c81dbe7a",
            "ecfc45ed98b745538feecb9a5312ef69",
            "f93cb79c011e43e8bd40fb87b93c45db",
            "c1d1cdf15e8c40fd8a0d242376ab1d70"
          ]
        },
        "id": "w2wdiUMOgULw",
        "outputId": "ba5ec2cb-e7d9-4f91-fe13-84cf18a14fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbbc3750f26d4943b8b388d678e74f5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a97b1b792c184ca3af77f4fbb7f52e8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2fa15fdf5ad4c9ab71499227b1b407c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9794cbcf53174838bdcadca7abacd91b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29742e3b7b9c49e1a98a6ae329a396ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ac3c56be3347a68f545ab3c81dbe7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecfc45ed98b745538feecb9a5312ef69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f93cb79c011e43e8bd40fb87b93c45db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1d1cdf15e8c40fd8a0d242376ab1d70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LIdocMXwv0mC"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Uni/Bird Exploration/CUB_200_2011/llava_captions.json\", \"r\") as f:\n",
        "    dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4eY3DB7VgO4T"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# class BirdDataset(Dataset):\n",
        "#     def __init__(self, dataset, root_dir=\"/content/drive/My Drive/Uni/Bird Exploration/CUB_200_2011/images/\"):\n",
        "#         self.data = list(dataset.values())\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transforms.Compose([\n",
        "#             transforms.Resize((224, 224)),\n",
        "#             transforms.ToTensor(),\n",
        "#         ])\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def chunk_text(self, text, chunk_size=77, overlap=10):\n",
        "#         \"\"\"Splits text into 77-token chunks for CLIP compatibility.\"\"\"\n",
        "#         tokens = processor.tokenizer(\n",
        "#             text, return_tensors=\"pt\", padding=False, truncation=True, max_length=chunk_size\n",
        "#         )[\"input_ids\"].squeeze()\n",
        "#         return tokens.unsqueeze(0)  # Ensure tensor format is correct\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         positive_sample = self.data[idx]\n",
        "\n",
        "#         # Load Image\n",
        "#         class_label = positive_sample[\"class_label\"]\n",
        "#         image_filename = os.path.basename(positive_sample[\"image_path\"])\n",
        "#         image_path = os.path.join(self.root_dir, class_label, image_filename)\n",
        "\n",
        "#         if not os.path.exists(image_path):\n",
        "#             image = torch.zeros((3, 224, 224))\n",
        "#         else:\n",
        "#             image = Image.open(image_path).convert(\"RGB\")\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         # Process text\n",
        "#         text = positive_sample.get(\"llava_text\", \"\").strip()\n",
        "#         if not text:\n",
        "#             text = \"No description available.\"\n",
        "\n",
        "#         text_chunks = self.chunk_text(text)\n",
        "#         encoded_texts = text_chunks  # No need to stack\n",
        "\n",
        "#         # Negative Sample (wrong bird text)\n",
        "#         neg_idx = np.random.randint(0, len(self.data))\n",
        "#         while neg_idx == idx:\n",
        "#             neg_idx = np.random.randint(0, len(self.data))\n",
        "\n",
        "#         negative_text = self.data[neg_idx].get(\"llava_text\", \"\").strip()\n",
        "#         if not negative_text:\n",
        "#             negative_text = \"No negative description available.\"\n",
        "\n",
        "#         negative_chunks = self.chunk_text(negative_text)\n",
        "#         encoded_negative_texts = negative_chunks\n",
        "\n",
        "#         return image, encoded_texts, encoded_negative_texts\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     images, text_chunks, neg_text_chunks = zip(*batch)\n",
        "\n",
        "#     images_tensor = torch.stack(images).to(device)\n",
        "\n",
        "#     # Pad sequences properly\n",
        "#     texts_tensor = pad_sequence(text_chunks, batch_first=True, padding_value=0).to(device)\n",
        "#     neg_texts_tensor = pad_sequence(neg_text_chunks, batch_first=True, padding_value=0).to(device)\n",
        "\n",
        "#     return images_tensor, texts_tensor, neg_texts_tensor\\\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, dataset, root_dir=\"/content/drive/My Drive/Uni/Bird Exploration/CUB_200_2011/images/\"):\n",
        "        self.data = list(dataset.values())\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def chunk_text(self, text, chunk_size=77, overlap=10):\n",
        "        \"\"\"Splits text into overlapping 77-token chunks for CLIP compatibility.\"\"\"\n",
        "        tokens = processor.tokenizer(\n",
        "            text, return_tensors=\"pt\", padding=False, truncation=False\n",
        "        )[\"input_ids\"].squeeze()\n",
        "\n",
        "        # Create overlapping chunks\n",
        "        chunks = []\n",
        "        for i in range(0, len(tokens), chunk_size - overlap):\n",
        "            chunk = tokens[i:i+chunk_size]\n",
        "            if len(chunk) < chunk_size:  # Pad shorter chunks\n",
        "                chunk = torch.cat([chunk, torch.zeros(chunk_size - len(chunk), dtype=torch.long)])\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks  # Return all chunks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      positive_sample = self.data[idx]\n",
        "\n",
        "      # Load Image\n",
        "      class_label = positive_sample[\"class_label\"]\n",
        "      image_filename = os.path.basename(positive_sample[\"image_path\"])\n",
        "      image_path = os.path.join(self.root_dir, class_label, image_filename)\n",
        "\n",
        "      if not os.path.exists(image_path):\n",
        "          image = torch.zeros((3, 224, 224))\n",
        "      else:\n",
        "          image = Image.open(image_path).convert(\"RGB\")\n",
        "          image = self.transform(image)\n",
        "\n",
        "      # Process text (Chunk LLAVA Captions Properly)\n",
        "      text = positive_sample.get(\"llava_text\", \"\").strip()\n",
        "      if not text:\n",
        "          text = \"No description available.\"\n",
        "\n",
        "      text_chunks = self.chunk_text(text)  # **Get multiple chunks**\n",
        "      encoded_texts = torch.stack(text_chunks)  # Convert to tensor\n",
        "\n",
        "      # Negative Sample (wrong bird text)\n",
        "      neg_idx = np.random.randint(0, len(self.data))\n",
        "      while neg_idx == idx:\n",
        "          neg_idx = np.random.randint(0, len(self.data))\n",
        "\n",
        "      negative_text = self.data[neg_idx].get(\"llava_text\", \"\").strip()\n",
        "      if not negative_text:\n",
        "          negative_text = \"No negative description available.\"\n",
        "\n",
        "      negative_chunks = self.chunk_text(negative_text)\n",
        "      encoded_negative_texts = torch.stack(negative_chunks)\n",
        "\n",
        "      return image, encoded_texts, encoded_negative_texts\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, text_chunks, neg_text_chunks = zip(*batch)\n",
        "\n",
        "    images_tensor = torch.stack(images).to(device)\n",
        "\n",
        "    # **Only Keep Necessary Tokens in Each Sequence**\n",
        "    max_length = min(77, max(chunk.shape[0] for chunks in text_chunks for chunk in chunks))\n",
        "    max_neg_length = min(77, max(chunk.shape[0] for chunks in neg_text_chunks for chunk in chunks))\n",
        "\n",
        "    texts_tensor = pad_sequence(\n",
        "        [chunk[:max_length] for chunks in text_chunks for chunk in chunks],\n",
        "        batch_first=True, padding_value=0\n",
        "    ).to(device)\n",
        "\n",
        "    neg_texts_tensor = pad_sequence(\n",
        "        [chunk[:max_neg_length] for chunks in neg_text_chunks for chunk in chunks],\n",
        "        batch_first=True, padding_value=0\n",
        "    ).to(device)\n",
        "\n",
        "    return images_tensor, texts_tensor, neg_texts_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chunk_text(self, text, chunk_size=77, overlap=10):\n",
        "    \"\"\"Splits text into overlapping 77-token chunks for CLIP compatibility.\"\"\"\n",
        "    tokens = processor.tokenizer(\n",
        "        text, return_tensors=\"pt\", padding=False, truncation=False\n",
        "    )[\"input_ids\"].squeeze()\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        if len(chunk) < chunk_size:  # Pad shorter chunks\n",
        "            chunk = torch.cat([chunk, torch.zeros(chunk_size - len(chunk), dtype=torch.long)])\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    # **Ensure Every Chunk is Exactly 77 Tokens Before Returning**\n",
        "    return [chunk[:chunk_size] for chunk in chunks]  # ✅ Now, no chunk exceeds 77 tokens\n",
        "\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     images, text_chunks, neg_text_chunks = zip(*batch)\n",
        "\n",
        "#     images_tensor = torch.stack(images).to(device)\n",
        "\n",
        "#     # Pad and batch all text chunks\n",
        "#     texts_tensor = pad_sequence([chunk for chunks in text_chunks for chunk in chunks], batch_first=True, padding_value=0).to(device)\n",
        "#     neg_texts_tensor = pad_sequence([chunk for chunks in neg_text_chunks for chunk in chunks], batch_first=True, padding_value=0).to(device)\n",
        "\n",
        "#     return images_tensor, texts_tensor, neg_texts_tensor\n",
        "\n",
        "\n",
        "\n",
        "# def chunk_text(self, text, chunk_size=77, overlap=10):\n",
        "#     \"\"\"Splits text into overlapping 77-token chunks for CLIP compatibility.\"\"\"\n",
        "#     tokens = processor.tokenizer(\n",
        "#         text, return_tensors=\"pt\", padding=False, truncation=False\n",
        "#     )[\"input_ids\"].squeeze()\n",
        "\n",
        "#     chunks = []\n",
        "#     for i in range(0, len(tokens), chunk_size - overlap):\n",
        "#         chunk = tokens[i:i + chunk_size]\n",
        "#         if len(chunk) < chunk_size:  # Pad shorter chunks\n",
        "#             chunk = torch.cat([chunk, torch.zeros(chunk_size - len(chunk), dtype=torch.long)])\n",
        "#         chunks.append(chunk)\n",
        "\n",
        "#     return chunks  # Returns all chunks (not just one)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsU6woWV8lHq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "wandb.init(\n",
        "    project=\"Bird Exploration\",  # Set a project name\n",
        "    name=\"CLIP_LoRA_Training_Run\",  # Run name\n",
        "    config={\"epochs\": 5, \"learning_rate\": 5e-5, \"batch_size\": 8}  # Track hyperparameters\n",
        ")\n",
        "\n",
        "\n",
        "# Enable CUDA Debugging\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# Load CLIP Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "clip_model = get_peft_model(clip_model, lora_config)\n",
        "clip_model.print_trainable_parameters()\n",
        "\n",
        "# ✅ Split Dataset into Train & Test (80/20 split)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# ✅ Convert dataset to list before splitting\n",
        "dataset_items = list(dataset.items())  # Convert dictionary to list for indexing\n",
        "train_data, test_data = random_split(dataset_items, [train_size, test_size])\n",
        "\n",
        "# ✅ Create BirdDataset instances from split data\n",
        "train_dataset = BirdDataset(dict(train_data))\n",
        "test_dataset = BirdDataset(dict(test_data))\n",
        "\n",
        "# ✅ Initialize Data Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)  # ✅ Shuffle ON for training\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)  # ✅ Shuffle OFF for testing\n",
        "\n",
        "# ✅ Optimizer & Contrastive Loss\n",
        "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()  # ✅ Use Contrastive Loss\n",
        "\n",
        "# ✅ Function to Compute Accuracy\n",
        "def compute_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, encoded_texts, _ in dataloader:\n",
        "            images, encoded_texts = images.to(device), encoded_texts.to(device)\n",
        "\n",
        "            # Get embeddings\n",
        "            outputs_text = model.get_text_features(input_ids=encoded_texts)\n",
        "            outputs_image = model.get_image_features(pixel_values=images)\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            similarities = F.cosine_similarity(outputs_text, outputs_image)\n",
        "\n",
        "            # Count correct predictions (Threshold = 0.5)\n",
        "            correct += (similarities > 0.5).sum().item()\n",
        "            total += len(similarities)\n",
        "\n",
        "    return correct / total  # Accuracy as percentage\n",
        "\n",
        "# ✅ Training Loop\n",
        "# ✅ Training Loop with `wandb` Logging\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    clip_model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, encoded_texts, encoded_neg_texts in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move to GPU\n",
        "        images, encoded_texts, encoded_neg_texts = images.to(device), encoded_texts.to(device), encoded_neg_texts.to(device)\n",
        "\n",
        "        # Forward pass (positive text pairs)\n",
        "        outputs_pos = clip_model.get_text_features(input_ids=encoded_texts)\n",
        "        outputs_img = clip_model.get_image_features(pixel_values=images)\n",
        "\n",
        "        # Forward pass (negative text pairs)\n",
        "        outputs_neg = clip_model.get_text_features(input_ids=encoded_neg_texts)\n",
        "\n",
        "        # **Ensure batch sizes are equal**\n",
        "        min_batch_size = min(outputs_pos.shape[0], outputs_img.shape[0])\n",
        "        outputs_pos, outputs_neg, outputs_img = outputs_pos[:min_batch_size], outputs_neg[:min_batch_size], outputs_img[:min_batch_size]\n",
        "\n",
        "        # Target Labels: Positive = 1, Negative = -1\n",
        "        labels = torch.cat([torch.ones(min_batch_size), -torch.ones(min_batch_size)]).to(device)\n",
        "\n",
        "        # Compute Contrastive Loss\n",
        "        loss = loss_fn(\n",
        "            torch.cat([outputs_pos, outputs_neg]),\n",
        "            torch.cat([outputs_img, outputs_img]),\n",
        "            labels\n",
        "        )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # ✅ Log Loss to `wandb`\n",
        "        wandb.log({\"Train Loss\": loss.item()})\n",
        "\n",
        "        # Update Progress Bar\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # ✅ Compute Accuracy & Log to `wandb`\n",
        "    train_acc = compute_accuracy(clip_model, train_loader)\n",
        "    test_acc = compute_accuracy(clip_model, test_loader)\n",
        "\n",
        "    wandb.log({\"Train Accuracy\": train_acc, \"Test Accuracy\": test_acc})\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f} - Train Acc: {train_acc:.4f} - Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# ✅ Finish `wandb` Logging\n",
        "wandb.finish()\n",
        "\n",
        "# ✅ Save CLIP Model & LoRA Weights\n",
        "save_path = \"clip_lora_trained\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "clip_model.save_pretrained(save_path)\n",
        "clip_model.save_adapter(save_path, \"lora\")\n",
        "processor.save_pretrained(save_path)\n",
        "\n",
        "print(f\"✅ Model and weights saved to {save_path}\")\n",
        "\n",
        "# ✅ Load CLIP Model & LoRA Weights for Inference\n",
        "def load_clip_lora():\n",
        "    clip_model = CLIPModel.from_pretrained(save_path).to(device)\n",
        "    clip_model = PeftModel.from_pretrained(clip_model, f\"{save_path}/lora\")\n",
        "    processor = CLIPProcessor.from_pretrained(save_path)\n",
        "    print(\"✅ Model and LoRA weights loaded successfully!\")\n",
        "    return clip_model, processor\n",
        "\n",
        "# ✅ Load & Test Model After Training\n",
        "clip_model, processor = load_clip_lora()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVz5nQzIhZRF"
      },
      "outputs": [],
      "source": [
        "# train_dataset = BirdDataset(dataset)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "# optimizer = torch.optim.Adam(clip_model.parameters(), lr=5e-5)\n",
        "# loss_fn = torch.nn.CosineEmbeddingLoss()  # Contrastive loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJLZT7Ddhb9K"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from peft import LoraConfig, get_peft_model\n",
        "# from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# # Enable CUDA Debugging\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "\n",
        "# # Restart CUDA\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Load CLIP Model\n",
        "# model_name = \"openai/clip-vit-base-patch32\"\n",
        "# clip_model = CLIPModel.from_pretrained(model_name)\n",
        "# clip_model.to(device)  # Move after loading\n",
        "# processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# # Apply LoRA\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.1,\n",
        "#     target_modules=[\"q_proj\", \"v_proj\"]\n",
        "# )\n",
        "# clip_model = get_peft_model(clip_model, lora_config)\n",
        "# clip_model.print_trainable_parameters()\n",
        "\n",
        "# # Optimizer & Training Setup\n",
        "# optimizer = torch.optim.AdamW(clip_model.parameters(), lr=5e-5)\n",
        "\n",
        "# # Training Loop\n",
        "# num_epochs = 5\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     clip_model.train()\n",
        "#     total_loss = 0\n",
        "#     for images, encoded_texts, encoded_neg_texts in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Move to GPU\n",
        "#         images = images.to(device)\n",
        "#         encoded_texts = encoded_texts.to(device)\n",
        "#         encoded_neg_texts = encoded_neg_texts.to(device)\n",
        "\n",
        "#         # Debugging: Print tensor shapes\n",
        "#         print(f\"Encoded texts shape: {encoded_texts.shape}\")  # Should be [batch, 77]\n",
        "#         print(f\"Negative texts shape: {encoded_neg_texts.shape}\")  # Should be [batch, 77]\n",
        "\n",
        "#         # Forward pass (positive pairs)\n",
        "#         outputs = clip_model(input_ids=encoded_texts, pixel_values=images)\n",
        "#         logits_per_image = outputs.logits_per_image\n",
        "\n",
        "#         # Ensure Correct Target Tensor\n",
        "#         targets = torch.arange(len(images), dtype=torch.long).to(device)\n",
        "#         print(f\"Targets shape: {targets.shape}\")  # Should match batch_size\n",
        "#         print(f\"Target values: {targets}\")  # Should be 0 to batch_size-1\n",
        "\n",
        "#         # Compute Loss\n",
        "#         loss = F.cross_entropy(logits_per_image, targets)\n",
        "\n",
        "#         # Forward pass (negative pairs)\n",
        "#         neg_outputs = clip_model(input_ids=encoded_neg_texts, pixel_values=images)\n",
        "#         neg_logits = neg_outputs.logits_per_image\n",
        "\n",
        "#         # Compute Negative Loss\n",
        "#         neg_loss = F.cross_entropy(neg_logits, targets)\n",
        "#         loss += neg_loss\n",
        "\n",
        "#         # Backpropagation\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# # Save LoRA fine-tuned CLIP\n",
        "# clip_model.save_pretrained(\"clip_lora_trained\")\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Enable CUDA Debugging\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# Load CLIP Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Apply Corrected LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "clip_model = get_peft_model(clip_model, lora_config)\n",
        "clip_model.print_trainable_parameters()\n",
        "\n",
        "# Optimizer & Contrastive Loss\n",
        "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()  # ✅ Use Contrastive Loss\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 5\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    clip_model.train()\n",
        "    total_loss = 0\n",
        "    for images, encoded_texts, encoded_neg_texts in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move to GPU\n",
        "        images = images.to(device)\n",
        "        encoded_texts = encoded_texts.to(device)\n",
        "        encoded_neg_texts = encoded_neg_texts.to(device)\n",
        "\n",
        "        # Forward pass (positive text pairs)\n",
        "        outputs_pos = clip_model(input_ids=encoded_texts, pixel_values=images)\n",
        "        embeddings_pos = outputs_pos.text_embeds\n",
        "\n",
        "        # Forward pass (negative text pairs)\n",
        "        outputs_neg = clip_model(input_ids=encoded_neg_texts, pixel_values=images)\n",
        "        embeddings_neg = outputs_neg.text_embeds\n",
        "\n",
        "        # Target Labels: Positive = 1, Negative = -1\n",
        "        labels = torch.cat([torch.ones(len(embeddings_pos)), -torch.ones(len(embeddings_neg))]).to(device)\n",
        "\n",
        "        # Compute Contrastive Loss\n",
        "        loss = loss_fn(torch.cat([embeddings_pos, embeddings_neg]), torch.cat([embeddings_pos, embeddings_neg]), labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Save LoRA fine-tuned CLIP\n",
        "clip_model.save_pretrained(\"clip_lora_trained\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gyi2Ruqg_l9"
      },
      "outputs": [],
      "source": [
        "def retrieve_image(query_text):\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model(**inputs).text_embeds\n",
        "\n",
        "    similarities = {}\n",
        "\n",
        "    for key, sample in dataset.items():\n",
        "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model(**inputs).image_embeds\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(text_features, image_features)\n",
        "        similarities[key] = similarity.item()\n",
        "\n",
        "    best_match = max(similarities, key=similarities.get)\n",
        "    return dataset[best_match][\"image_path\"]\n",
        "\n",
        "# Example test\n",
        "query_text = \"A cliff swallow with a red forehead perched on a wooden post.\"\n",
        "print(retrieve_image(query_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx6AYzb9vYrh"
      },
      "outputs": [],
      "source": [
        "def retrieve_text(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(**image_inputs)\n",
        "\n",
        "    similarities = {}\n",
        "\n",
        "    for key, sample in dataset.items():\n",
        "        text = sample[\"llava_text\"]\n",
        "        text_inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = clip_model.get_text_features(**text_inputs)\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
        "        similarities[key] = similarity.item()\n",
        "\n",
        "    best_match = max(similarities, key=similarities.get)\n",
        "    return dataset[best_match][\"llava_text\"]\n",
        "\n",
        "test_image = \"/kaggle/input/cub2002011/CUB_200_2011/images/002.Laysan_Albatross/Laysan_Albatross_0001_545.jpg\"\n",
        "print(retrieve_text(test_image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nujqgmm1zcVZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2534241,
          "sourceId": 5140550,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6885461,
          "sourceId": 11052109,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "cvEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}