{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHdbgIf5gsRS"
      },
      "outputs": [],
      "source": [
        "?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_DU95chf0uX",
        "outputId": "7713cde5-5d68-4e0b-bdc8-46ee7d2735c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install --upgrade -q accelerate bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2wdiUMOgULw"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAkhn4J5hU2-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply LoRA to CLIP\n",
        "config = LoraConfig(\n",
        "    r=8,  # Low-rank factor\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"visual_projection\", \"text_projection\"],  # LoRA for vision & text\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "clip_model = get_peft_model(clip_model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eY3DB7VgO4T"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load dataset from JSON file\n",
        "with open(\"dataset.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Custom Dataset Class with Hard Negatives\n",
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.data = list(dataset.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        positive_sample = self.data[idx]\n",
        "\n",
        "        # Load positive image\n",
        "        image = Image.open(positive_sample[\"image_path\"]).convert(\"RGB\")\n",
        "\n",
        "        # Text Description (Llava-generated)\n",
        "        text = positive_sample[\"llava_text\"]\n",
        "\n",
        "        # Select a negative sample (wrong bird text)\n",
        "        neg_idx = np.random.randint(0, len(self.data))\n",
        "        while neg_idx == idx:  # Avoid picking the same sample\n",
        "            neg_idx = np.random.randint(0, len(self.data))\n",
        "        negative_text = self.data[neg_idx][\"llava_text\"]\n",
        "\n",
        "        return image, text, negative_text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVz5nQzIhZRF"
      },
      "outputs": [],
      "source": [
        "# DataLoader\n",
        "train_dataset = BirdDataset(dataset)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Training Configuration\n",
        "optimizer = torch.optim.Adam(clip_model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()  # Contrastive loss\n",
        "epochs = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4sAv-HGg-ep"
      },
      "outputs": [],
      "source": [
        "def retrieve_text(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        image_features = outputs.image_embeds\n",
        "\n",
        "    similarities = {}\n",
        "\n",
        "    for key, sample in dataset.items():\n",
        "        text = sample[\"llava_text\"]\n",
        "        inputs = processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = clip_model(**inputs).text_embeds\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
        "        similarities[key] = similarity.item()\n",
        "\n",
        "    best_match = max(similarities, key=similarities.get)\n",
        "    return dataset[best_match][\"llava_text\"]\n",
        "\n",
        "# Example test\n",
        "test_image = \"path/to/test/image.jpg\"\n",
        "print(retrieve_text(test_image))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJLZT7Ddhb9K"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "clip_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "    for images, texts, negative_texts in progress_bar:\n",
        "        inputs_pos = processor(text=texts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "        inputs_neg = processor(text=negative_texts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_pos = clip_model(**inputs_pos)\n",
        "        outputs_neg = clip_model(**inputs_neg)\n",
        "\n",
        "        image_features = outputs_pos.image_embeds\n",
        "        text_features = outputs_pos.text_embeds\n",
        "        neg_text_features = outputs_neg.text_embeds\n",
        "\n",
        "        # Compute loss (positive pair should be close, negative should be far)\n",
        "        target_pos = torch.ones(image_features.shape[0]).to(device)  # Positive pair target\n",
        "        target_neg = -torch.ones(image_features.shape[0]).to(device)  # Negative pair target\n",
        "\n",
        "        loss_pos = loss_fn(image_features, text_features, target_pos)\n",
        "        loss_neg = loss_fn(image_features, neg_text_features, target_neg)\n",
        "\n",
        "        loss = loss_pos + loss_neg  # Combine losses\n",
        "\n",
        "        # Accuracy: Positive similarity > Negative similarity\n",
        "        pos_sim = torch.nn.functional.cosine_similarity(image_features, text_features).mean().item()\n",
        "        neg_sim = torch.nn.functional.cosine_similarity(image_features, neg_text_features).mean().item()\n",
        "        accuracy = (pos_sim > neg_sim).sum().item() / image_features.shape[0]\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        total_loss += loss.item()\n",
        "        total_correct += accuracy\n",
        "        total_samples += 1\n",
        "\n",
        "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"Acc\": f\"{accuracy:.2%}\", \"Pos Sim\": f\"{pos_sim:.3f}\", \"Neg Sim\": f\"{neg_sim:.3f}\"})\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = total_correct / total_samples\n",
        "    print(f\"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}, Avg Accuracy = {avg_acc:.2%}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "clip_model.save_pretrained(\"clip_lora_bird_retrieval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gyi2Ruqg_l9"
      },
      "outputs": [],
      "source": [
        "def retrieve_image(query_text):\n",
        "    inputs = processor(text=[query_text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model(**inputs).text_embeds\n",
        "\n",
        "    similarities = {}\n",
        "\n",
        "    for key, sample in dataset.items():\n",
        "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model(**inputs).image_embeds\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(text_features, image_features)\n",
        "        similarities[key] = similarity.item()\n",
        "\n",
        "    best_match = max(similarities, key=similarities.get)\n",
        "    return dataset[best_match][\"image_path\"]\n",
        "\n",
        "# Example test\n",
        "query_text = \"A cliff swallow with a red forehead perched on a wooden post.\"\n",
        "print(retrieve_image(query_text))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
