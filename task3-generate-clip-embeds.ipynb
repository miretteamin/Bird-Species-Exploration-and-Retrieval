{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\Anaconda\\envs\\clip\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id                                          file_path\n",
      "0         1  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "1         2  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "2         3  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "3         4  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "4         5  001.Black_footed_Albatross/Black_Footed_Albatr...\n",
      "   image_id  class_id\n",
      "0         1         1\n",
      "1         2         1\n",
      "2         3         1\n",
      "3         4         1\n",
      "4         5         1\n",
      "   class_id                  class_name\n",
      "0         1  001.Black_footed_Albatross\n",
      "1         2        002.Laysan_Albatross\n",
      "2         3         003.Sooty_Albatross\n",
      "3         4       004.Groove_billed_Ani\n",
      "4         5          005.Crested_Auklet\n",
      "(11788, 2)\n",
      "(11788, 2)\n",
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the CUB-200-2011 dataset\n",
    "def load_cub_dataset(data_dir):\n",
    "    images = pd.read_csv(os.path.join(data_dir, 'images.txt'), sep=' ', names=['image_id', 'file_path'])\n",
    "    labels = pd.read_csv(os.path.join(data_dir, 'image_class_labels.txt'), sep=' ', names=['image_id', 'class_id'])\n",
    "    classes = pd.read_csv(os.path.join(data_dir, 'classes.txt'), sep=' ', names=['class_id', 'class_name'])\n",
    "    bounding_boxes = pd.read_csv(os.path.join(data_dir, 'bounding_boxes.txt'), sep=' ', names=['image_id', 'x', 'y', 'width', 'height'])\n",
    "    part_locs = pd.read_csv(os.path.join(data_dir, 'parts/part_locs.txt'), sep=' ', names=['img_id', 'part_id', 'x', 'y', 'visible'])\n",
    "    # parts = pd.read_csv(os.path.join(data_dir, 'parts/parts.txt'), delimiter =' ', names=['part_id', 'part_name'])\n",
    "    parts = pd.read_fwf(os.path.join(data_dir, 'parts/parts.txt'), colspecs=[(0, 2), (2, None)], header=None, names=['part_id', 'part_name'])\n",
    "    parts_click_locs = pd.read_csv(os.path.join(data_dir, 'parts/part_click_locs.txt'), sep = ' ', names=['image_id', 'part_id', 'x', 'y', 'visible', 'time'])\n",
    "    attributes = pd.read_csv(os.path.join(data_dir, 'attributes/attributes.txt'), sep = ' ', names=['attribute_id', 'attribute_name'])\n",
    "    certainties = pd.read_fwf(os.path.join(data_dir, 'attributes/certainties.txt'), colspecs=[(0, 1), (2, None)], names=[\"certainty_id\", \"certainty_name\"])\n",
    "    image_attribute_labels = pd.read_csv(os.path.join(data_dir, 'attributes/image_attribute_labels.txt'), sep = ' ', names=['image_id', 'attribute_id', 'is_present', 'certainty_id', 'time'])\n",
    "    with open(os.path.join(data_dir, 'llava_captions.json'), 'r') as f:\n",
    "        llava_captions = json.load(f)\n",
    "    return images, labels, classes,  bounding_boxes, parts, part_locs, parts_click_locs, attributes, certainties, image_attribute_labels, llava_captions\n",
    "data_dir = 'data'\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "parts_dir = os.path.join(data_dir, 'parts')\n",
    "\n",
    "images, labels, classes, bounding_boxes, parts, part_locs, parts_click_locs, attributes, certainties, image_attribute_labels, llava_captions = load_cub_dataset(data_dir)\n",
    "\n",
    "print(images.head())\n",
    "print(labels.head())\n",
    "print(classes.head())\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.481, 0.457, 0.408), std=(0.268, 0.261, 0.275))\n",
    "])\n",
    "\n",
    "def get_clip_img_features(img_path, transform):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    # image = transform(image)\n",
    "    # image = transforms.ToPILImage()(image)\n",
    "    image = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_features = clip_model.encode_image(image).cpu().numpy()\n",
    "    return img_features\n",
    "\n",
    "def get_clip_text_features(text):\n",
    "    text_inputs = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_inputs).cpu().numpy()\n",
    "    return text_features\n",
    "\n",
    "def get_cosine_similarity(img_features: torch.Tensor, txt_features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between image and text feature tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    img_features (torch.Tensor): Feature tensor for the image.\n",
    "    txt_features (torch.Tensor): Feature tensor for the text.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Cosine similarity score between -1 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    img_features = img_features.squeeze()\n",
    "    txt_features = txt_features.squeeze()\n",
    "    \n",
    "    # Compute dot product\n",
    "    dot_product = torch.dot(img_features, txt_features)\n",
    "    \n",
    "    # Compute L2 norms\n",
    "    norm_img = torch.norm(img_features, p=2)\n",
    "    norm_txt = torch.norm(txt_features, p=2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if norm_img == 0 or norm_txt == 0:\n",
    "        return torch.tensor(0.0)  # Handle zero-vector cases\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = dot_product / (norm_img * norm_txt)\n",
    "    \n",
    "    return similarity.item()\n",
    "\n",
    "def get_clip_llava_caption_features(text):\n",
    "    sentences = [sentence.strip() for sentence in text.split('.') if sentence.strip()]\n",
    "    text_features = np.zeros((1, 512))\n",
    "    for sentence in sentences:\n",
    "        text_features += get_clip_text_features(sentence)\n",
    "    text_features /= len(sentences)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11788/11788 [13:41<00:00, 14.36it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "def generate_clip_embeddings_imgs(data_dir, transform):\n",
    "    img_dir = os.path.join(data_dir, 'images')    \n",
    "    clip_embeds = np.zeros((11788, 512))\n",
    "    for img_idx in tqdm(range(1, 11789)):\n",
    "        img_path = images[images['image_id'] == img_idx]['file_path'].iloc[0]\n",
    "        clip_embeds[img_idx - 1, :] = get_clip_img_features(os.path.join(img_dir, img_path), transform).squeeze()\n",
    "    return clip_embeds\n",
    "clip_embeds_imgs = generate_clip_embeddings_imgs(data_dir, data_transform)\n",
    "\n",
    "np.save('./data/clip_embeds_imgs2.npy', clip_embeds_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11788/11788 [1:41:10<00:00,  1.94it/s] \n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "def generate_clip_embeddings_text(data_dir):\n",
    "    clip_embeds = np.zeros((11788, 512))\n",
    "    for img_idx in tqdm(range(1, 11789)):\n",
    "        llava_caption = llava_captions[str(img_idx)]['llava_text']\n",
    "        clip_embeds[img_idx-1, :] = get_clip_llava_caption_features(llava_caption)\n",
    "    return clip_embeds\n",
    "clip_embeds_text = generate_clip_embeddings_text(data_dir)\n",
    "\n",
    "np.save('./data/clip_embeds_text.npy', clip_embeds_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
