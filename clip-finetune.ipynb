{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T20:37:23.043696Z",
     "iopub.status.busy": "2025-03-17T20:37:23.043396Z",
     "iopub.status.idle": "2025-03-17T20:37:23.047181Z",
     "shell.execute_reply": "2025-03-17T20:37:23.046265Z",
     "shell.execute_reply.started": "2025-03-17T20:37:23.043667Z"
    },
    "id": "jHdbgIf5gsRS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# dataset_path = \"/content/drive/My Drive/Uni/Bird Exploration/CUB_200_2011\"\n",
    "\n",
    "# if os.path.exists(dataset_path):\n",
    "#   print(f\"âœ… CUB dataset is available at: {dataset_path}\")\n",
    "# else:\n",
    "#   print(\"âŒ Dataset not found! Run the download script first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-17T21:19:51.804988Z",
     "iopub.status.busy": "2025-03-17T21:19:51.804725Z",
     "iopub.status.idle": "2025-03-17T21:20:56.190932Z",
     "shell.execute_reply": "2025-03-17T21:20:56.190248Z",
     "shell.execute_reply.started": "2025-03-17T21:19:51.804964Z"
    },
    "id": "U_DU95chf0uX",
    "outputId": "7713cde5-5d68-4e0b-bdc8-46ee7d2735c9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: nltk in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (3.9.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wcwidth in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: click in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (4.50.0.dev0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: safetensors in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\miret\\AppData\\Local\\Temp\\pip-req-build-oy0kh6t7'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\miret\\appdata\\local\\temp\\pip-req-build-oy0kh6t7\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 7f5077e53682ca855afc826162b204ebf809f1f9\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from tqdm>=4.27->transformers==4.50.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\miret\\anaconda3\\envs\\cvenv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\miret\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm nltk peft\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install --upgrade -q accelerate bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T21:20:56.192533Z",
     "iopub.status.busy": "2025-03-17T21:20:56.191870Z",
     "iopub.status.idle": "2025-03-17T21:21:07.528877Z",
     "shell.execute_reply": "2025-03-17T21:21:07.527947Z",
     "shell.execute_reply.started": "2025-03-17T21:20:56.192505Z"
    },
    "id": "w2wdiUMOgULw",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\miret\\anaconda3\\envs\\cvEnv\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\miret\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T21:21:07.530239Z",
     "iopub.status.busy": "2025-03-17T21:21:07.529972Z",
     "iopub.status.idle": "2025-03-17T21:21:08.011002Z",
     "shell.execute_reply": "2025-03-17T21:21:08.010359Z",
     "shell.execute_reply.started": "2025-03-17T21:21:07.530215Z"
    },
    "id": "4eY3DB7VgO4T",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/llava_captions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/llava_captions.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBirdDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[1;32md:\\Users\\miret\\anaconda3\\envs\\cvEnv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/llava_captions.json'"
     ]
    }
   ],
   "source": [
    "with open(\"/data/llava_captions.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, dataset, root_dir=\"/kaggle/input/cub2002011/CUB_200_2011/images/\"):\n",
    "        self.data = list(dataset.values())\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  \n",
    "            transforms.ToTensor(),  \n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        positive_sample = self.data[idx]\n",
    "        \n",
    "        # Extract image path and class label\n",
    "        class_label = positive_sample[\"class_label\"]\n",
    "        image_filename = os.path.basename(positive_sample[\"image_path\"])\n",
    "        image_path = os.path.join(self.root_dir, class_label, image_filename)\n",
    "\n",
    "        # Handle missing images\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Missing image {image_path}. Using blank tensor.\")\n",
    "            image = torch.zeros((3, 224, 224))  # Return blank tensor instead of breaking\n",
    "        else:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process text: Split into sentences and tokenize\n",
    "        text = positive_sample.get(\"llava_text\", \"\").strip()\n",
    "        if not text:\n",
    "            text = \"No description available.\"\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "        # Convert each sentence into tokenized tensors\n",
    "        encoded_text = processor(text=sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "\n",
    "        # Select a negative sample (wrong bird text)\n",
    "        neg_idx = np.random.randint(0, len(self.data))\n",
    "        while neg_idx == idx:\n",
    "            neg_idx = np.random.randint(0, len(self.data))\n",
    "        negative_text = self.data[neg_idx].get(\"llava_text\", \"\").strip()\n",
    "        if not negative_text:\n",
    "            negative_text = \"No negative description available.\"\n",
    "        negative_sentences = nltk.tokenize.sent_tokenize(negative_text)\n",
    "\n",
    "        # Convert negative text into tokenized tensors\n",
    "        encoded_negative_text = processor(text=negative_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "\n",
    "        return image, encoded_text, encoded_negative_text\n",
    "\n",
    "# **ðŸ”¹ Custom Collate Function with Dynamic Padding**\n",
    "def collate_fn(batch):\n",
    "    images, encoded_texts, encoded_neg_texts = zip(*batch)\n",
    "\n",
    "    # Convert images to tensor batch\n",
    "    images = torch.stack(images).to(device)\n",
    "\n",
    "    # **Find the longest sequence length in the batch**\n",
    "    max_length = max([et[\"input_ids\"].shape[1] for et in encoded_texts])\n",
    "    max_neg_length = max([et[\"input_ids\"].shape[1] for et in encoded_neg_texts])\n",
    "\n",
    "    # **Pad input sequences to max length**\n",
    "    def pad_sequence(seq_list, max_len):\n",
    "        batch_size = len(seq_list)\n",
    "        padded_seq = torch.zeros((batch_size, max_len), dtype=torch.long).to(device)\n",
    "        attention_masks = torch.zeros((batch_size, max_len), dtype=torch.long).to(device)\n",
    "\n",
    "        for i, et in enumerate(seq_list):\n",
    "            length = et[\"input_ids\"].shape[1]\n",
    "            padded_seq[i, :length] = et[\"input_ids\"].squeeze(0)\n",
    "            attention_masks[i, :length] = et[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\"input_ids\": padded_seq, \"attention_mask\": attention_masks}\n",
    "\n",
    "    # Pad positive and negative texts\n",
    "    padded_texts = pad_sequence(encoded_texts, max_length)\n",
    "    padded_neg_texts = pad_sequence(encoded_neg_texts, max_neg_length)\n",
    "\n",
    "    return images, padded_texts, padded_neg_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T21:21:08.012527Z",
     "iopub.status.busy": "2025-03-17T21:21:08.012251Z",
     "iopub.status.idle": "2025-03-17T21:21:08.018318Z",
     "shell.execute_reply": "2025-03-17T21:21:08.017589Z",
     "shell.execute_reply.started": "2025-03-17T21:21:08.012506Z"
    },
    "id": "OVz5nQzIhZRF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = BirdDataset(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CosineEmbeddingLoss()  # Contrastive loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T20:38:34.542908Z",
     "iopub.status.busy": "2025-03-17T20:38:34.542673Z",
     "iopub.status.idle": "2025-03-17T20:38:34.887098Z",
     "shell.execute_reply": "2025-03-17T20:38:34.885999Z",
     "shell.execute_reply.started": "2025-03-17T20:38:34.542888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"visual_projection\", \"text_projection\"],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "clip_model = get_peft_model(clip_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T20:38:34.888357Z",
     "iopub.status.busy": "2025-03-17T20:38:34.888130Z",
     "iopub.status.idle": "2025-03-17T20:38:35.516634Z",
     "shell.execute_reply": "2025-03-17T20:38:35.514971Z",
     "shell.execute_reply.started": "2025-03-17T20:38:34.888339Z"
    },
    "id": "bJLZT7Ddhb9K",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/2947 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-4-3350f20a207d>\", line 60, in collate_fn\n    images = torch.stack(images).to(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7b385fb71224>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_neg_texts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-4-3350f20a207d>\", line 60, in collate_fn\n    images = torch.stack(images).to(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BirdDataset(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    clip_model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, encoded_texts, encoded_neg_texts in progress_bar:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Convert tokenized text to device\n",
    "        input_ids = encoded_texts[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded_texts[\"attention_mask\"].to(device)\n",
    "\n",
    "        neg_input_ids = encoded_neg_texts[\"input_ids\"].to(device)\n",
    "        neg_attention_mask = encoded_neg_texts[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Process text embeddings\n",
    "        text_embeddings = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        neg_text_embeddings = clip_model.get_text_features(input_ids=neg_input_ids, attention_mask=neg_attention_mask)\n",
    "\n",
    "        # Compute similarity loss\n",
    "        labels = torch.ones(text_embeddings.shape[0]).to(device)  # Positive samples\n",
    "        neg_labels = -torch.ones(neg_text_embeddings.shape[0]).to(device)  # Negative samples\n",
    "\n",
    "        loss = loss_fn(text_embeddings, neg_text_embeddings, labels) + loss_fn(neg_text_embeddings, text_embeddings, neg_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(clip_model.state_dict(), \"clip_finetuned_lora.pth\")\n",
    "print(\"âœ… Training complete. Model saved as clip_finetuned_lora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gyi2Ruqg_l9"
   },
   "outputs": [],
   "source": [
    "def retrieve_image(query_text):\n",
    "    inputs = processor(text=[query_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model(**inputs).text_embeds\n",
    "\n",
    "    similarities = {}\n",
    "\n",
    "    for key, sample in dataset.items():\n",
    "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model(**inputs).image_embeds\n",
    "\n",
    "        similarity = torch.nn.functional.cosine_similarity(text_features, image_features)\n",
    "        similarities[key] = similarity.item()\n",
    "\n",
    "    best_match = max(similarities, key=similarities.get)\n",
    "    return dataset[best_match][\"image_path\"]\n",
    "\n",
    "# Example test\n",
    "query_text = \"A cliff swallow with a red forehead perched on a wooden post.\"\n",
    "print(retrieve_image(query_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_text(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**image_inputs)\n",
    "\n",
    "    similarities = {}\n",
    "\n",
    "    for key, sample in dataset.items():\n",
    "        text = sample[\"llava_text\"]\n",
    "        text_inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "        similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
    "        similarities[key] = similarity.item()\n",
    "\n",
    "    best_match = max(similarities, key=similarities.get)\n",
    "    return dataset[best_match][\"llava_text\"]\n",
    "\n",
    "test_image = \"/kaggle/input/cub2002011/CUB_200_2011/images/002.Laysan_Albatross/Laysan_Albatross_0001_545.jpg\"\n",
    "print(retrieve_text(test_image))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2534241,
     "sourceId": 5140550,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6885461,
     "sourceId": 11052109,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cvEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
